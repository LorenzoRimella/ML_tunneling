{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook require an installation of \"lazypredict-nightly\", which can be install as follows in a conda environment as follows:\n",
    "- conda create --n ML python=3.9\n",
    "- conda activate ML\n",
    "- pip install lazypredict-nightly\n",
    "- pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"../data/Training_Dataset_2.xlsx\")\n",
    "\n",
    "data = data.values[:,1:]\n",
    "\n",
    "X = data[:,:-1].astype(float)\n",
    "mu = np.mean(X, axis = 0)\n",
    "std = np.std(X, axis = 0)\n",
    "X = (X - mu)/std\n",
    "\n",
    "y = data[:,-1]\n",
    "\n",
    "y = np.where(y==\"E\",      np.zeros(y.shape), y)\n",
    "y = np.where(y==\"L\",  1 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"L \", 1 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"R\",  2 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"S\",  3 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"W\",  4 + np.zeros(y.shape), y)\n",
    "\n",
    "y = y.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy predict for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict import LazyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Line\n",
    "clf = LazyClassifier(verbose=0,\n",
    "                     ignore_warnings=True,\n",
    "                     custom_metric=None,\n",
    "                     predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_lazy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m models, predictions \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_lazy\u001b[49m, X_test_lazy, y_train_lazy, y_test_lazy)\n\u001b[1;32m      2\u001b[0m models\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_lazy' is not defined"
     ]
    }
   ],
   "source": [
    "models, predictions = clf.fit(X_train_lazy, X_test_lazy, y_train_lazy, y_test_lazy)\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster_list = [\"gbtree\"]\n",
    "\n",
    "n_estimators_list  = [25, 50, 100, 150]\n",
    "\n",
    "eta_list = [0.01, 0.1, 1.]\n",
    "max_depth_list = [5, 10, 30]\n",
    "\n",
    "lambda_list = [0.0, 5.0, 10.]\n",
    "\n",
    "max_leaves_list   = [0, 1, 5]\n",
    "\n",
    "# accuracy_vector = -100 + np.zeros((len(booster_list), len(n_estimators_list), len(eta_list), len(max_depth_list), \n",
    "#                                    len(lambda_list), len(max_leaves_list) ))\n",
    "\n",
    "# for i1 in range(len(booster_list)):\n",
    "#     for i2 in range(len(n_estimators_list)):\n",
    "#         for i3 in range(len(eta_list)):\n",
    "#             for i4 in range(len(max_depth_list)):\n",
    "#                 for i5 in range(len(lambda_list)):\n",
    "#                         for i7 in range(len(max_leaves_list)):\n",
    "\n",
    "#                                 clf = XGBClassifier(booster = booster_list[i1], \n",
    "#                                                     n_estimators = n_estimators_list[i2], \n",
    "#                                                     eta = eta_list[i3], \n",
    "#                                                     max_depth = max_depth_list[i4], \n",
    "#                                                     reg_lambda = lambda_list[i5], \n",
    "#                                                     max_leaves = max_leaves_list[i7],  \n",
    "#                                                     random_state = 42)\n",
    "                                \n",
    "#                                 model = clf.fit(X_train, y_train)\n",
    "\n",
    "#                                 # Predict the labels for the test set\n",
    "#                                 y_pred = model.predict(X_test)\n",
    "\n",
    "#                                 # Calculate accuracy\n",
    "#                                 accuracy_vector[i1, i2, i3, i4, i5, i7] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#                                 print(\"#####################\")\n",
    "#                                 print(\"booster \", booster_list[i1])\n",
    "#                                 print(\"n_estimators \", n_estimators_list[i2])\n",
    "#                                 print(\"eta \", eta_list[i3])\n",
    "#                                 print(\"max_depth \", max_depth_list[i4])\n",
    "#                                 print(\"lambda \", lambda_list[i5])\n",
    "#                                 print(\"max_leaves \", max_leaves_list[i7])\n",
    "#                                 print(accuracy_vector[i1, i2, i3, i4, i5, i7])\n",
    "\n",
    "# np.save(\"accuracy_XGB.npy\", accuracy_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vector_XGB = np.load(\"data/accuracy_XGB.npy\")\n",
    "max_index = np.unravel_index(np.argmax(accuracy_vector_XGB), accuracy_vector_XGB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946961894953656"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(booster = booster_list[max_index[0]], \n",
    "                    n_estimators = n_estimators_list[max_index[1]], \n",
    "                    eta = eta_list[max_index[2]], \n",
    "                    max_depth = max_depth_list[max_index[3]], \n",
    "                    reg_lambda = lambda_list[max_index[4]], \n",
    "                    max_leaves = max_leaves_list[max_index[5]],  )\n",
    "\n",
    "XGB = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = XGB.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreesClassifier\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_list  = [25, 50, 100, 150]\n",
    "\n",
    "min_samples_split_list = [1, 2, 5]\n",
    "max_depth_list = [5, 10, 20, 30]\n",
    "\n",
    "min_samples_leaf_list = [1, 2, 5]\n",
    "criterion_list  = [\"gini\", \"entropy\"]\n",
    "\n",
    "\n",
    "# accuracy_vector_extra_tree = -100 + np.zeros((len(n_estimators_list), len(min_samples_split_list), len(max_depth_list), \n",
    "#                                               len(min_samples_leaf_list), len(criterion_list) ))\n",
    "\n",
    "# for i1 in range(len(n_estimators_list)):\n",
    "#     for i2 in range(len(min_samples_split_list)):\n",
    "#         for i3 in range(len(max_depth_list)):\n",
    "#             for i4 in range(len(min_samples_leaf_list)):\n",
    "#                 for i5 in range(len(criterion_list)):\n",
    "\n",
    "#                                 clf = ExtraTreesClassifier( n_estimators = n_estimators_list[i1], \n",
    "#                                                             min_samples_split = min_samples_split_list[i2], \n",
    "#                                                             max_depth = max_depth_list[i3], \n",
    "#                                                             min_samples_leaf = min_samples_leaf_list[i4], \n",
    "#                                                             criterion = criterion_list[i5],  \n",
    "#                                                             random_state = 42)\n",
    "                                \n",
    "#                                 model = clf.fit(X_train, y_train)\n",
    "\n",
    "#                                 # Predict the labels for the test set\n",
    "#                                 y_pred = model.predict(X_test)\n",
    "\n",
    "#                                 # Calculate accuracy\n",
    "#                                 accuracy_vector_extra_tree[i1, i2, i3, i4, i5] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#                                 print(\"#####################\")\n",
    "#                                 print(\"n_estimators\", n_estimators_list[i1])\n",
    "#                                 print(\"min_samples_split\", min_samples_split_list[i2])\n",
    "#                                 print(\"max_depth\", max_depth_list[i3])\n",
    "#                                 print(\"min_samples_leaf\", min_samples_leaf_list[i4])\n",
    "#                                 print(\"criterion\", criterion_list[i5])\n",
    "#                                 print(accuracy_vector_extra_tree[i1, i2, i3, i4, i5])\n",
    "\n",
    "# np.save(\"accuracy_ExtraTree.npy\", accuracy_vector_extra_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vector_ExtraTree = np.load(\"data/accuracy_ExtraTree.npy\")\n",
    "max_index = np.unravel_index(np.argmax(accuracy_vector_ExtraTree), accuracy_vector_ExtraTree.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9500514933058702"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ExtraTreesClassifier( n_estimators = n_estimators_list[max_index[0]], \n",
    "                            min_samples_split = min_samples_split_list[max_index[1]], \n",
    "                            max_depth = max_depth_list[max_index[2]], \n",
    "                            min_samples_leaf = min_samples_leaf_list[max_index[3]], \n",
    "                            criterion = criterion_list[max_index[4]],  )\n",
    "\n",
    "ExtraTree = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = ExtraTree.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_leaves_list  = [10, 30 , 50]\n",
    "\n",
    "n_estimators_list  = [25, 50, 100, 150]\n",
    "\n",
    "learning_rate_list  = [0.01, 0.1, 1]\n",
    "max_depth_list = [5, 10, 20, 30]\n",
    "\n",
    "lambda_list = [0.0, 1.0, 5.0]\n",
    "boosting_type_list  = [\"gbdt\"]\n",
    "\n",
    "# accuracy_vector = -100 + np.zeros((len(num_leaves_list), len(n_estimators_list), len(learning_rate_list), len(max_depth_list), \n",
    "#                                    len(lambda_list), len(boosting_type_list) ))\n",
    "\n",
    "# for i1 in range(len(num_leaves_list)):\n",
    "#     for i2 in range(len(n_estimators_list)):\n",
    "#         for i3 in range(len(learning_rate_list)):\n",
    "#             for i4 in range(len(max_depth_list)):\n",
    "#                 for i5 in range(len(lambda_list)):\n",
    "#                     for i6 in range(len(boosting_type_list)):\n",
    "\n",
    "#                                 clf = LGBMClassifier(num_leaves = num_leaves_list[i1], \n",
    "#                                                     n_estimators = n_estimators_list[i2], \n",
    "#                                                     learning_rate = learning_rate_list[i3], \n",
    "#                                                     max_depth = max_depth_list[i4], \n",
    "#                                                     reg_lambda = lambda_list[i5], \n",
    "#                                                     boosting_type = boosting_type_list[i6],   \n",
    "#                                                     random_state=42)\n",
    "                                \n",
    "#                                 model = clf.fit(X_train, y_train)\n",
    "\n",
    "#                                 # Predict the labels for the test set\n",
    "#                                 y_pred = model.predict(X_test)\n",
    "\n",
    "#                                 # Calculate accuracy\n",
    "#                                 accuracy_vector[i1, i2, i3, i4, i5, i6] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#                                 print(\"#####################\")\n",
    "#                                 print(\"booster \", boosting_type_list[i6])\n",
    "#                                 print(\"n_estimators \", n_estimators_list[i2])\n",
    "#                                 print(\"learning rate \", learning_rate_list[i3])\n",
    "#                                 print(\"max_depth \", max_depth_list[i4])\n",
    "#                                 print(\"lambda \", lambda_list[i5])\n",
    "#                                 print(\"num_leaves \", num_leaves_list[i1])\n",
    "#                                 print(accuracy_vector[i1, i2, i3, i4, i5, i6])\n",
    "\n",
    "# np.save(\"accuracy_LGBM.npy\", accuracy_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vector_LGBM = np.load(\"data/accuracy_LGBM.npy\")\n",
    "max_index = np.unravel_index(np.argmax(accuracy_vector_LGBM), accuracy_vector_LGBM.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9495365602471678"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LGBMClassifier(num_leaves = num_leaves_list[max_index[0]], \n",
    "                    n_estimators = n_estimators_list[max_index[1]], \n",
    "                    learning_rate = learning_rate_list[max_index[2]], \n",
    "                    max_depth = max_depth_list[max_index[3]], \n",
    "                    reg_lambda = lambda_list[max_index[4]], \n",
    "                    boosting_type = boosting_type_list[max_index[5]], \n",
    "                    force_col_wise=True, verbose = -1   )\n",
    "\n",
    "LGBM = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = LGBM.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= []\n",
    "\n",
    "n_estimators_list = [25, 50, 100, 150]\n",
    "criterion_list = [\"gini\", \"entropy\"]\n",
    "min_samples_leaf_list = [1, 2, 5, 10]\n",
    "max_depth_list = [None, 5, 10, 20, 30, 50]\n",
    "\n",
    "# accuracy_vector = -100 + np.zeros((len(n_estimators_list), len(criterion_list), len(min_samples_leaf_list), len(max_depth_list)))\n",
    "\n",
    "# for i1 in range(len(n_estimators_list)):\n",
    "#     for i2 in range(len(criterion_list)):\n",
    "#         for i3 in range(len(min_samples_leaf_list)):\n",
    "#             for i4 in range(len(max_depth_list)):\n",
    "#                     clf = RandomForestClassifier(n_estimators = n_estimators_list[i1], \n",
    "#                                                  criterion    = criterion_list[i2], \n",
    "#                                                  min_samples_leaf = min_samples_leaf_list[i3], \n",
    "#                                                  max_depth        = max_depth_list[i4],\n",
    "#                                                  random_state=42\n",
    "#                                                  )\n",
    "#                     model = clf.fit(X_train, y_train)\n",
    "\n",
    "#                     # Predict the labels for the test set\n",
    "#                     y_pred = model.predict(X_test)\n",
    "\n",
    "#                     # Calculate accuracy\n",
    "#                     accuracy_vector[i1, i2, i3, i4] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#                     print(\"#####################\")\n",
    "#                     print(\"n_estimators \", n_estimators_list[i1])\n",
    "#                     print(\"criterion \", criterion_list[i2])\n",
    "#                     print(\"min sample leaf \", min_samples_leaf_list[i3])\n",
    "#                     print(\"max_depth \", max_depth_list[i4])\n",
    "#                     print(accuracy_vector[i1, i2, i3, i4])\n",
    "\n",
    "# np.save(\"accuracy_RadomForest.npy\", accuracy_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vector_RandomForest = np.load(\"data/accuracy_RadomForest.npy\")\n",
    "max_index = np.unravel_index(np.argmax(accuracy_vector_RandomForest), accuracy_vector_RandomForest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9387229660144182"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier( n_estimators     = n_estimators_list[max_index[0]], \n",
    "                              criterion        = criterion_list[max_index[1]], \n",
    "                              min_samples_leaf = min_samples_leaf_list[max_index[2]], \n",
    "                              max_depth        = max_depth_list[max_index[3]]\n",
    "                            )\n",
    "\n",
    "RF = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = RF.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9495365602471678"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB_pred = XGB.predict(X_test) \n",
    "ExtraTree_pred = ExtraTree.predict(X_test) \n",
    "LGBM_pred = LGBM.predict(X_test) \n",
    "RF_pred = RF.predict(X_test) \n",
    "\n",
    "predictions = np.stack((XGB_pred, ExtraTree_pred, LGBM_pred, RF_pred), axis = 1)\n",
    "values      = np.unique(predictions)\n",
    "\n",
    "boolean_check = np.expand_dims(predictions, axis = -1) == np.expand_dims(values, axis = (0, 1))\n",
    "weights = np.expand_dims(np.expand_dims(np.array([1.5, 1.5, 0.5, 0.5]), axis = 0), axis = -1)\n",
    "ensemble_prediction_prob = np.mean(weights*boolean_check.astype(float), axis = -2)\n",
    "\n",
    "Y_pred_ensemble = np.argmax(ensemble_prediction_prob, axis = 1)\n",
    "\n",
    "accuracy_score(y_test, Y_pred_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the notebook requires an installation of tensorflow, follow the instructions from https://www.tensorflow.org/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"../data/Training_Dataset_2.xlsx\")\n",
    "\n",
    "data = data.values[:,1:]\n",
    "\n",
    "X = data[:,:-1].astype(float)\n",
    "mu = np.mean(X, axis = 0)\n",
    "std = np.std(X, axis = 0)\n",
    "X = (X - mu)/std\n",
    "\n",
    "y = data[:,-1]\n",
    "\n",
    "y = np.where(y==\"E\",      np.zeros(y.shape), y)\n",
    "y = np.where(y==\"L\",  1 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"L \", 1 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"R\",  2 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"S\",  3 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"W\",  4 + np.zeros(y.shape), y)\n",
    "\n",
    "y = y.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Dense(4096, activation='relu'),\n",
    "#     # tf.keras.layers.Dropout(0.3),  # Add dropout with a dropout rate of 0.5 (you can adjust this value)\n",
    "#     tf.keras.layers.Dense(4096, activation='relu'),\n",
    "#     # tf.keras.layers.Dropout(0.3),  # Add dropout with a dropout rate of 0.5 (you can adjust this value)\n",
    "#     tf.keras.layers.Dense(5, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=500) #, validation_data=(X_test, y_test))\n",
    "\n",
    "# model.save('NeuralNet.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File too big to be uploaded contact the authors if interested\n",
    "model = tf.keras.models.load_model('NeuralNet.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/61 [===================>..........] - ETA: 0s - loss: 0.7380 - accuracy: 0.9435"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 17:24:03.622784: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 0s 3ms/step - loss: 0.6976 - accuracy: 0.9413\n",
      "Test accuracy: 0.9413\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
