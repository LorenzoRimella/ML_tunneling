{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook require an installation of \"lazypredict-nightly\", which can be install as follows in a conda environment as follows:\n",
    "- conda create --n ML python=3.9\n",
    "- conda activate ML\n",
    "- pip install lazypredict-nightly\n",
    "- pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"../data/Training_Dataset_2.xlsx\")\n",
    "\n",
    "data = data.values[:,1:]\n",
    "\n",
    "X = data[:,:-1].astype(float)\n",
    "mu = np.mean(X, axis = 0)\n",
    "std = np.std(X, axis = 0)\n",
    "X = (X - mu)/std\n",
    "\n",
    "y = data[:,-1]\n",
    "\n",
    "y = np.where(y==\"E\",      np.zeros(y.shape), y)\n",
    "y = np.where(y==\"L\",  1 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"L \", 1 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"R\",  2 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"S\",  3 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"W\",  4 + np.zeros(y.shape), y)\n",
    "\n",
    "y = y.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy predict for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict import LazyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Line\n",
    "clf = LazyClassifier(verbose=0,\n",
    "                     ignore_warnings=True,\n",
    "                     custom_metric=None,\n",
    "                     predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:03<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 3171, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -1.024775\n",
      "[LightGBM] [Info] Start training from score -1.285295\n",
      "[LightGBM] [Info] Start training from score -3.217615\n",
      "[LightGBM] [Info] Start training from score -1.605033\n",
      "[LightGBM] [Info] Start training from score -2.090540\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "      <td>None</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.90</td>\n",
       "      <td>None</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesClassifier</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "      <td>None</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.89</td>\n",
       "      <td>None</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingClassifier</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.87</td>\n",
       "      <td>None</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelPropagation</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.85</td>\n",
       "      <td>None</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelSpreading</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.85</td>\n",
       "      <td>None</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.83</td>\n",
       "      <td>None</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.82</td>\n",
       "      <td>None</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.82</td>\n",
       "      <td>None</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.74</td>\n",
       "      <td>None</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.73</td>\n",
       "      <td>None</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.73</td>\n",
       "      <td>None</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearestCentroid</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.57</td>\n",
       "      <td>None</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.57</td>\n",
       "      <td>None</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.56</td>\n",
       "      <td>None</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.55</td>\n",
       "      <td>None</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveClassifier</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.54</td>\n",
       "      <td>None</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.53</td>\n",
       "      <td>None</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CalibratedClassifierCV</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>None</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearDiscriminantAnalysis</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.51</td>\n",
       "      <td>None</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.51</td>\n",
       "      <td>None</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.46</td>\n",
       "      <td>None</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifier</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.43</td>\n",
       "      <td>None</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifierCV</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.43</td>\n",
       "      <td>None</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.20</td>\n",
       "      <td>None</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  Balanced Accuracy ROC AUC  F1 Score  \\\n",
       "Model                                                                          \n",
       "XGBClassifier                      0.93               0.90    None      0.93   \n",
       "LGBMClassifier                     0.92               0.90    None      0.92   \n",
       "ExtraTreesClassifier               0.93               0.90    None      0.93   \n",
       "RandomForestClassifier             0.92               0.89    None      0.92   \n",
       "BaggingClassifier                  0.90               0.87    None      0.90   \n",
       "LabelPropagation                   0.89               0.85    None      0.89   \n",
       "LabelSpreading                     0.89               0.85    None      0.89   \n",
       "DecisionTreeClassifier             0.87               0.83    None      0.87   \n",
       "ExtraTreeClassifier                0.85               0.82    None      0.85   \n",
       "KNeighborsClassifier               0.86               0.82    None      0.86   \n",
       "QuadraticDiscriminantAnalysis      0.69               0.74    None      0.65   \n",
       "GaussianNB                         0.67               0.73    None      0.62   \n",
       "SVC                                0.82               0.73    None      0.82   \n",
       "NearestCentroid                    0.55               0.57    None      0.54   \n",
       "LogisticRegression                 0.70               0.57    None      0.66   \n",
       "SGDClassifier                      0.67               0.56    None      0.63   \n",
       "BernoulliNB                        0.65               0.55    None      0.61   \n",
       "PassiveAggressiveClassifier        0.62               0.54    None      0.58   \n",
       "AdaBoostClassifier                 0.47               0.53    None      0.34   \n",
       "CalibratedClassifierCV             0.68               0.53    None      0.62   \n",
       "LinearDiscriminantAnalysis         0.66               0.51    None      0.60   \n",
       "LinearSVC                          0.66               0.51    None      0.59   \n",
       "Perceptron                         0.56               0.46    None      0.55   \n",
       "RidgeClassifier                    0.61               0.43    None      0.50   \n",
       "RidgeClassifierCV                  0.61               0.43    None      0.50   \n",
       "DummyClassifier                    0.35               0.20    None      0.18   \n",
       "\n",
       "                               Time Taken  \n",
       "Model                                      \n",
       "XGBClassifier                        0.33  \n",
       "LGBMClassifier                       0.19  \n",
       "ExtraTreesClassifier                 0.17  \n",
       "RandomForestClassifier               0.55  \n",
       "BaggingClassifier                    0.15  \n",
       "LabelPropagation                     0.26  \n",
       "LabelSpreading                       0.35  \n",
       "DecisionTreeClassifier               0.03  \n",
       "ExtraTreeClassifier                  0.01  \n",
       "KNeighborsClassifier                 0.04  \n",
       "QuadraticDiscriminantAnalysis        0.01  \n",
       "GaussianNB                           0.01  \n",
       "SVC                                  0.14  \n",
       "NearestCentroid                      0.11  \n",
       "LogisticRegression                   0.03  \n",
       "SGDClassifier                        0.04  \n",
       "BernoulliNB                          0.01  \n",
       "PassiveAggressiveClassifier          0.01  \n",
       "AdaBoostClassifier                   0.19  \n",
       "CalibratedClassifierCV               0.06  \n",
       "LinearDiscriminantAnalysis           0.10  \n",
       "LinearSVC                            0.21  \n",
       "Perceptron                           0.01  \n",
       "RidgeClassifier                      0.01  \n",
       "RidgeClassifierCV                    0.04  \n",
       "DummyClassifier                      0.01  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_lazy, X_test_lazy, y_train_lazy, y_test_lazy = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "models, predictions = clf.fit(X_train_lazy, X_test_lazy, y_train_lazy, y_test_lazy)\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster_list = [\"gbtree\"]\n",
    "\n",
    "n_estimators_list  = [25, 50, 100, 150]\n",
    "\n",
    "eta_list = [0.01, 0.1, 1.]\n",
    "max_depth_list = [5, 10, 30]\n",
    "\n",
    "lambda_list = [0.0, 5.0, 10.]\n",
    "\n",
    "max_leaves_list   = [0, 1, 5]\n",
    "\n",
    "# accuracy_vector = -100 + np.zeros((len(booster_list), len(n_estimators_list), len(eta_list), len(max_depth_list), \n",
    "#                                    len(lambda_list), len(max_leaves_list) ))\n",
    "\n",
    "# for i1 in range(len(booster_list)):\n",
    "#     for i2 in range(len(n_estimators_list)):\n",
    "#         for i3 in range(len(eta_list)):\n",
    "#             for i4 in range(len(max_depth_list)):\n",
    "#                 for i5 in range(len(lambda_list)):\n",
    "#                         for i7 in range(len(max_leaves_list)):\n",
    "\n",
    "#                                 clf = XGBClassifier(booster = booster_list[i1], \n",
    "#                                                     n_estimators = n_estimators_list[i2], \n",
    "#                                                     eta = eta_list[i3], \n",
    "#                                                     max_depth = max_depth_list[i4], \n",
    "#                                                     reg_lambda = lambda_list[i5], \n",
    "#                                                     max_leaves = max_leaves_list[i7],  \n",
    "#                                                     random_state = 42)\n",
    "                                \n",
    "#                                 model = clf.fit(X_train, y_train)\n",
    "\n",
    "#                                 # Predict the labels for the test set\n",
    "#                                 y_pred = model.predict(X_test)\n",
    "\n",
    "#                                 # Calculate accuracy\n",
    "#                                 accuracy_vector[i1, i2, i3, i4, i5, i7] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#                                 print(\"#####################\")\n",
    "#                                 print(\"booster \", booster_list[i1])\n",
    "#                                 print(\"n_estimators \", n_estimators_list[i2])\n",
    "#                                 print(\"eta \", eta_list[i3])\n",
    "#                                 print(\"max_depth \", max_depth_list[i4])\n",
    "#                                 print(\"lambda \", lambda_list[i5])\n",
    "#                                 print(\"max_leaves \", max_leaves_list[i7])\n",
    "#                                 print(accuracy_vector[i1, i2, i3, i4, i5, i7])\n",
    "\n",
    "# np.save(\"accuracy_XGB.npy\", accuracy_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vector_XGB = np.load(\"data/accuracy_XGB.npy\")\n",
    "max_index = np.unravel_index(np.argmax(accuracy_vector_XGB), accuracy_vector_XGB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946961894953656"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(booster = booster_list[max_index[0]], \n",
    "                    n_estimators = n_estimators_list[max_index[1]], \n",
    "                    eta = eta_list[max_index[2]], \n",
    "                    max_depth = max_depth_list[max_index[3]], \n",
    "                    reg_lambda = lambda_list[max_index[4]], \n",
    "                    max_leaves = max_leaves_list[max_index[5]],  )\n",
    "\n",
    "XGB = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = XGB.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreesClassifier\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_list  = [25, 50, 100, 150]\n",
    "\n",
    "min_samples_split_list = [1, 2, 5]\n",
    "max_depth_list = [5, 10, 20, 30]\n",
    "\n",
    "min_samples_leaf_list = [1, 2, 5]\n",
    "criterion_list  = [\"gini\", \"entropy\"]\n",
    "\n",
    "\n",
    "# accuracy_vector_extra_tree = -100 + np.zeros((len(n_estimators_list), len(min_samples_split_list), len(max_depth_list), \n",
    "#                                               len(min_samples_leaf_list), len(criterion_list) ))\n",
    "\n",
    "# for i1 in range(len(n_estimators_list)):\n",
    "#     for i2 in range(len(min_samples_split_list)):\n",
    "#         for i3 in range(len(max_depth_list)):\n",
    "#             for i4 in range(len(min_samples_leaf_list)):\n",
    "#                 for i5 in range(len(criterion_list)):\n",
    "\n",
    "#                                 clf = ExtraTreesClassifier( n_estimators = n_estimators_list[i1], \n",
    "#                                                             min_samples_split = min_samples_split_list[i2], \n",
    "#                                                             max_depth = max_depth_list[i3], \n",
    "#                                                             min_samples_leaf = min_samples_leaf_list[i4], \n",
    "#                                                             criterion = criterion_list[i5],  \n",
    "#                                                             random_state = 42)\n",
    "                                \n",
    "#                                 model = clf.fit(X_train, y_train)\n",
    "\n",
    "#                                 # Predict the labels for the test set\n",
    "#                                 y_pred = model.predict(X_test)\n",
    "\n",
    "#                                 # Calculate accuracy\n",
    "#                                 accuracy_vector_extra_tree[i1, i2, i3, i4, i5] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#                                 print(\"#####################\")\n",
    "#                                 print(\"n_estimators\", n_estimators_list[i1])\n",
    "#                                 print(\"min_samples_split\", min_samples_split_list[i2])\n",
    "#                                 print(\"max_depth\", max_depth_list[i3])\n",
    "#                                 print(\"min_samples_leaf\", min_samples_leaf_list[i4])\n",
    "#                                 print(\"criterion\", criterion_list[i5])\n",
    "#                                 print(accuracy_vector_extra_tree[i1, i2, i3, i4, i5])\n",
    "\n",
    "# np.save(\"accuracy_ExtraTree.npy\", accuracy_vector_extra_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vector_ExtraTree = np.load(\"data/accuracy_ExtraTree.npy\")\n",
    "max_index = np.unravel_index(np.argmax(accuracy_vector_ExtraTree), accuracy_vector_ExtraTree.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9479917610710608"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ExtraTreesClassifier( n_estimators = n_estimators_list[max_index[0]], \n",
    "                            min_samples_split = min_samples_split_list[max_index[1]], \n",
    "                            max_depth = max_depth_list[max_index[2]], \n",
    "                            min_samples_leaf = min_samples_leaf_list[max_index[3]], \n",
    "                            criterion = criterion_list[max_index[4]],  )\n",
    "\n",
    "ExtraTree = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = ExtraTree.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_leaves_list  = [10, 30 , 50]\n",
    "\n",
    "n_estimators_list  = [25, 50, 100, 150]\n",
    "\n",
    "learning_rate_list  = [0.01, 0.1, 1]\n",
    "max_depth_list = [5, 10, 20, 30]\n",
    "\n",
    "lambda_list = [0.0, 1.0, 5.0]\n",
    "boosting_type_list  = [\"gbdt\"]\n",
    "\n",
    "# accuracy_vector = -100 + np.zeros((len(num_leaves_list), len(n_estimators_list), len(learning_rate_list), len(max_depth_list), \n",
    "#                                    len(lambda_list), len(boosting_type_list) ))\n",
    "\n",
    "# for i1 in range(len(num_leaves_list)):\n",
    "#     for i2 in range(len(n_estimators_list)):\n",
    "#         for i3 in range(len(learning_rate_list)):\n",
    "#             for i4 in range(len(max_depth_list)):\n",
    "#                 for i5 in range(len(lambda_list)):\n",
    "#                     for i6 in range(len(boosting_type_list)):\n",
    "\n",
    "#                                 clf = LGBMClassifier(num_leaves = num_leaves_list[i1], \n",
    "#                                                     n_estimators = n_estimators_list[i2], \n",
    "#                                                     learning_rate = learning_rate_list[i3], \n",
    "#                                                     max_depth = max_depth_list[i4], \n",
    "#                                                     reg_lambda = lambda_list[i5], \n",
    "#                                                     boosting_type = boosting_type_list[i6],   \n",
    "#                                                     random_state=42)\n",
    "                                \n",
    "#                                 model = clf.fit(X_train, y_train)\n",
    "\n",
    "#                                 # Predict the labels for the test set\n",
    "#                                 y_pred = model.predict(X_test)\n",
    "\n",
    "#                                 # Calculate accuracy\n",
    "#                                 accuracy_vector[i1, i2, i3, i4, i5, i6] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#                                 print(\"#####################\")\n",
    "#                                 print(\"booster \", boosting_type_list[i6])\n",
    "#                                 print(\"n_estimators \", n_estimators_list[i2])\n",
    "#                                 print(\"learning rate \", learning_rate_list[i3])\n",
    "#                                 print(\"max_depth \", max_depth_list[i4])\n",
    "#                                 print(\"lambda \", lambda_list[i5])\n",
    "#                                 print(\"num_leaves \", num_leaves_list[i1])\n",
    "#                                 print(accuracy_vector[i1, i2, i3, i4, i5, i6])\n",
    "\n",
    "# np.save(\"accuracy_LGBM.npy\", accuracy_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vector_LGBM = np.load(\"data/accuracy_LGBM.npy\")\n",
    "max_index = np.unravel_index(np.argmax(accuracy_vector_LGBM), accuracy_vector_LGBM.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9495365602471678"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LGBMClassifier(num_leaves = num_leaves_list[max_index[0]], \n",
    "                    n_estimators = n_estimators_list[max_index[1]], \n",
    "                    learning_rate = learning_rate_list[max_index[2]], \n",
    "                    max_depth = max_depth_list[max_index[3]], \n",
    "                    reg_lambda = lambda_list[max_index[4]], \n",
    "                    boosting_type = boosting_type_list[max_index[5]], \n",
    "                    force_col_wise=True, verbose = -1   )\n",
    "\n",
    "LGBM = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = LGBM.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= []\n",
    "\n",
    "n_estimators_list = [25, 50, 100, 150]\n",
    "criterion_list = [\"gini\", \"entropy\"]\n",
    "min_samples_leaf_list = [1, 2, 5, 10]\n",
    "max_depth_list = [None, 5, 10, 20, 30, 50]\n",
    "\n",
    "# accuracy_vector = -100 + np.zeros((len(n_estimators_list), len(criterion_list), len(min_samples_leaf_list), len(max_depth_list)))\n",
    "\n",
    "# for i1 in range(len(n_estimators_list)):\n",
    "#     for i2 in range(len(criterion_list)):\n",
    "#         for i3 in range(len(min_samples_leaf_list)):\n",
    "#             for i4 in range(len(max_depth_list)):\n",
    "#                     clf = RandomForestClassifier(n_estimators = n_estimators_list[i1], \n",
    "#                                                  criterion    = criterion_list[i2], \n",
    "#                                                  min_samples_leaf = min_samples_leaf_list[i3], \n",
    "#                                                  max_depth        = max_depth_list[i4],\n",
    "#                                                  random_state=42\n",
    "#                                                  )\n",
    "#                     model = clf.fit(X_train, y_train)\n",
    "\n",
    "#                     # Predict the labels for the test set\n",
    "#                     y_pred = model.predict(X_test)\n",
    "\n",
    "#                     # Calculate accuracy\n",
    "#                     accuracy_vector[i1, i2, i3, i4] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#                     print(\"#####################\")\n",
    "#                     print(\"n_estimators \", n_estimators_list[i1])\n",
    "#                     print(\"criterion \", criterion_list[i2])\n",
    "#                     print(\"min sample leaf \", min_samples_leaf_list[i3])\n",
    "#                     print(\"max_depth \", max_depth_list[i4])\n",
    "#                     print(accuracy_vector[i1, i2, i3, i4])\n",
    "\n",
    "# np.save(\"accuracy_RadomForest.npy\", accuracy_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vector_RandomForest = np.load(\"data/accuracy_RadomForest.npy\")\n",
    "max_index = np.unravel_index(np.argmax(accuracy_vector_RandomForest), accuracy_vector_RandomForest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9392378990731205"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier( n_estimators     = n_estimators_list[max_index[0]], \n",
    "                              criterion        = criterion_list[max_index[1]], \n",
    "                              min_samples_leaf = min_samples_leaf_list[max_index[2]], \n",
    "                              max_depth        = max_depth_list[max_index[3]]\n",
    "                            )\n",
    "\n",
    "RF = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = RF.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9505664263645726"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB_pred = XGB.predict(X_test) \n",
    "ExtraTree_pred = ExtraTree.predict(X_test) \n",
    "LGBM_pred = LGBM.predict(X_test) \n",
    "RF_pred = RF.predict(X_test) \n",
    "\n",
    "predictions = np.stack((XGB_pred, ExtraTree_pred, LGBM_pred, RF_pred), axis = 1)\n",
    "values      = np.unique(predictions)\n",
    "\n",
    "boolean_check = np.expand_dims(predictions, axis = -1) == np.expand_dims(values, axis = (0, 1))\n",
    "weights = np.expand_dims(np.expand_dims(np.array([1., 1., 1., 1.]), axis = 0), axis = -1)\n",
    "ensemble_prediction_prob = np.mean(weights*boolean_check.astype(float), axis = -2)\n",
    "\n",
    "Y_pred_ensemble = np.argmax(ensemble_prediction_prob, axis = 1)\n",
    "\n",
    "accuracy_score(y_test, Y_pred_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the notebook requires an installation of tensorflow, follow the instructions from https://www.tensorflow.org/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"../data/Training_Dataset_2.xlsx\")\n",
    "\n",
    "data = data.values[:,1:]\n",
    "\n",
    "X = data[:,:-1].astype(float)\n",
    "mu = np.mean(X, axis = 0)\n",
    "std = np.std(X, axis = 0)\n",
    "X = (X - mu)/std\n",
    "\n",
    "y = data[:,-1]\n",
    "\n",
    "y = np.where(y==\"E\",      np.zeros(y.shape), y)\n",
    "y = np.where(y==\"L\",  1 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"L \", 1 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"R\",  2 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"S\",  3 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"W\",  4 + np.zeros(y.shape), y)\n",
    "\n",
    "y = y.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Dense(4096, activation='relu'),\n",
    "#     # tf.keras.layers.Dropout(0.3),  # Add dropout with a dropout rate of 0.5 (you can adjust this value)\n",
    "#     tf.keras.layers.Dense(4096, activation='relu'),\n",
    "#     # tf.keras.layers.Dropout(0.3),  # Add dropout with a dropout rate of 0.5 (you can adjust this value)\n",
    "#     tf.keras.layers.Dense(5, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=500) #, validation_data=(X_test, y_test))\n",
    "\n",
    "# model.save('NeuralNet.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File too big to be uploaded contact the authors if interested\n",
    "model = tf.keras.models.load_model('NeuralNet.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/61 [===================>..........] - ETA: 0s - loss: 0.7380 - accuracy: 0.9435"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 17:24:03.622784: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 0s 3ms/step - loss: 0.6976 - accuracy: 0.9413\n",
      "Test accuracy: 0.9413\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
