{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"../data/Training_Dataset_2.xlsx\")\n",
    "\n",
    "data = data.values[:,1:]\n",
    "\n",
    "X = data[:,:-1].astype(float)\n",
    "mu = np.mean(X, axis = 0)\n",
    "std = np.std(X, axis = 0)\n",
    "X = (X - mu)/std\n",
    "\n",
    "y = data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.where(y==\"E\",      np.zeros(y.shape), y)\n",
    "y = np.where(y==\"L\",  1 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"L \", 1 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"R\",  2 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"S\",  3 + np.zeros(y.shape), y)\n",
    "y = np.where(y==\"W\",  4 + np.zeros(y.shape), y)\n",
    "\n",
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy predict for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Line\n",
    "clf = LazyClassifier(verbose=0,\n",
    "                     ignore_warnings=True,\n",
    "                     custom_metric=None,\n",
    "                     predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 28/29 [00:19<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000173 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 4531, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -1.032227\n",
      "[LightGBM] [Info] Start training from score -1.278245\n",
      "[LightGBM] [Info] Start training from score -3.277034\n",
      "[LightGBM] [Info] Start training from score -1.595412\n",
      "[LightGBM] [Info] Start training from score -2.081872\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:20<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.92</td>\n",
       "      <td>None</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesClassifier</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.92</td>\n",
       "      <td>None</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.92</td>\n",
       "      <td>None</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.92</td>\n",
       "      <td>None</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingClassifier</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.91</td>\n",
       "      <td>None</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelPropagation</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.90</td>\n",
       "      <td>None</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelSpreading</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.89</td>\n",
       "      <td>None</td>\n",
       "      <td>0.92</td>\n",
       "      <td>3.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>None</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.87</td>\n",
       "      <td>None</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.82</td>\n",
       "      <td>None</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.80</td>\n",
       "      <td>None</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.74</td>\n",
       "      <td>None</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.71</td>\n",
       "      <td>None</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.63</td>\n",
       "      <td>None</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearestCentroid</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.57</td>\n",
       "      <td>None</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.56</td>\n",
       "      <td>None</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CalibratedClassifierCV</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.53</td>\n",
       "      <td>None</td>\n",
       "      <td>0.61</td>\n",
       "      <td>4.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.51</td>\n",
       "      <td>None</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearDiscriminantAnalysis</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.51</td>\n",
       "      <td>None</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.49</td>\n",
       "      <td>None</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.49</td>\n",
       "      <td>None</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.48</td>\n",
       "      <td>None</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifier</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.44</td>\n",
       "      <td>None</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifierCV</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.44</td>\n",
       "      <td>None</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveClassifier</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.42</td>\n",
       "      <td>None</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.20</td>\n",
       "      <td>None</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  Balanced Accuracy ROC AUC  F1 Score  \\\n",
       "Model                                                                          \n",
       "XGBClassifier                      0.95               0.92    None      0.95   \n",
       "ExtraTreesClassifier               0.94               0.92    None      0.94   \n",
       "LGBMClassifier                     0.95               0.92    None      0.95   \n",
       "RandomForestClassifier             0.94               0.92    None      0.94   \n",
       "BaggingClassifier                  0.93               0.91    None      0.93   \n",
       "LabelPropagation                   0.92               0.90    None      0.92   \n",
       "LabelSpreading                     0.92               0.89    None      0.92   \n",
       "DecisionTreeClassifier             0.89               0.88    None      0.89   \n",
       "KNeighborsClassifier               0.90               0.87    None      0.90   \n",
       "ExtraTreeClassifier                0.86               0.82    None      0.86   \n",
       "SVC                                0.87               0.80    None      0.87   \n",
       "QuadraticDiscriminantAnalysis      0.69               0.74    None      0.65   \n",
       "GaussianNB                         0.65               0.71    None      0.60   \n",
       "AdaBoostClassifier                 0.55               0.63    None      0.46   \n",
       "NearestCentroid                    0.54               0.57    None      0.53   \n",
       "LogisticRegression                 0.70               0.56    None      0.65   \n",
       "CalibratedClassifierCV             0.67               0.53    None      0.61   \n",
       "LinearSVC                          0.65               0.51    None      0.58   \n",
       "LinearDiscriminantAnalysis         0.66               0.51    None      0.60   \n",
       "BernoulliNB                        0.62               0.49    None      0.58   \n",
       "Perceptron                         0.58               0.49    None      0.58   \n",
       "SGDClassifier                      0.63               0.48    None      0.56   \n",
       "RidgeClassifier                    0.60               0.44    None      0.49   \n",
       "RidgeClassifierCV                  0.60               0.44    None      0.49   \n",
       "PassiveAggressiveClassifier        0.47               0.42    None      0.46   \n",
       "DummyClassifier                    0.34               0.20    None      0.17   \n",
       "\n",
       "                               Time Taken  \n",
       "Model                                      \n",
       "XGBClassifier                        1.25  \n",
       "ExtraTreesClassifier                 0.79  \n",
       "LGBMClassifier                       0.76  \n",
       "RandomForestClassifier               2.18  \n",
       "BaggingClassifier                    0.90  \n",
       "LabelPropagation                     1.21  \n",
       "LabelSpreading                       3.11  \n",
       "DecisionTreeClassifier               0.13  \n",
       "KNeighborsClassifier                 0.17  \n",
       "ExtraTreeClassifier                  0.03  \n",
       "SVC                                  1.22  \n",
       "QuadraticDiscriminantAnalysis        0.04  \n",
       "GaussianNB                           0.02  \n",
       "AdaBoostClassifier                   1.15  \n",
       "NearestCentroid                      0.33  \n",
       "LogisticRegression                   0.28  \n",
       "CalibratedClassifierCV               4.19  \n",
       "LinearSVC                            1.54  \n",
       "LinearDiscriminantAnalysis           0.15  \n",
       "BernoulliNB                          0.17  \n",
       "Perceptron                           0.04  \n",
       "SGDClassifier                        0.13  \n",
       "RidgeClassifier                      0.04  \n",
       "RidgeClassifierCV                    0.04  \n",
       "PassiveAggressiveClassifier          0.07  \n",
       "DummyClassifier                      0.04  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster_list = [\"gbtree\"]\n",
    "\n",
    "n_estimators_list  = [25, 50, 100, 150]\n",
    "\n",
    "eta_list = [0.01, 0.1, 1.]\n",
    "max_depth_list = [5, 10, 30]\n",
    "\n",
    "lambda_list = [0.0, 5.0, 10.]\n",
    "\n",
    "max_leaves_list   = [0, 1, 5]\n",
    "\n",
    "# accuracy_vector = -100 + np.zeros((len(booster_list), len(n_estimators_list), len(eta_list), len(max_depth_list), \n",
    "#                                    len(lambda_list), len(max_leaves_list) ))\n",
    "\n",
    "# for i1 in range(len(booster_list)):\n",
    "#     for i2 in range(len(n_estimators_list)):\n",
    "#         for i3 in range(len(eta_list)):\n",
    "#             for i4 in range(len(max_depth_list)):\n",
    "#                 for i5 in range(len(lambda_list)):\n",
    "#                         for i7 in range(len(max_leaves_list)):\n",
    "\n",
    "#                                 clf = XGBClassifier(booster = booster_list[i1], \n",
    "#                                                     n_estimators = n_estimators_list[i2], \n",
    "#                                                     eta = eta_list[i3], \n",
    "#                                                     max_depth = max_depth_list[i4], \n",
    "#                                                     reg_lambda = lambda_list[i5], \n",
    "#                                                     max_leaves = max_leaves_list[i7],  \n",
    "#                                                     random_state = 42)\n",
    "                                \n",
    "#                                 model = clf.fit(X_train, y_train)\n",
    "\n",
    "#                                 # Predict the labels for the test set\n",
    "#                                 y_pred = model.predict(X_test)\n",
    "\n",
    "#                                 # Calculate accuracy\n",
    "#                                 accuracy_vector[i1, i2, i3, i4, i5, i7] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#                                 print(\"#####################\")\n",
    "#                                 print(\"booster \", booster_list[i1])\n",
    "#                                 print(\"n_estimators \", n_estimators_list[i2])\n",
    "#                                 print(\"eta \", eta_list[i3])\n",
    "#                                 print(\"max_depth \", max_depth_list[i4])\n",
    "#                                 print(\"lambda \", lambda_list[i5])\n",
    "#                                 print(\"max_leaves \", max_leaves_list[i7])\n",
    "#                                 print(accuracy_vector[i1, i2, i3, i4, i5, i7])\n",
    "\n",
    "# np.save(\"accuracy_XGB.npy\", accuracy_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vector_XGB = np.load(\"accuracy_XGB.npy\")\n",
    "max_index = np.unravel_index(np.argmax(accuracy_vector_XGB), accuracy_vector_XGB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946961894953656"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(booster = booster_list[max_index[0]], \n",
    "                    n_estimators = n_estimators_list[max_index[1]], \n",
    "                    eta = eta_list[max_index[2]], \n",
    "                    max_depth = max_depth_list[max_index[3]], \n",
    "                    reg_lambda = lambda_list[max_index[4]], \n",
    "                    max_leaves = max_leaves_list[max_index[5]],  )\n",
    "\n",
    "model = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreesClassifier\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_list  = [25, 50, 100, 150]\n",
    "\n",
    "min_samples_split_list = [1, 2, 5]\n",
    "max_depth_list = [5, 10, 20, 30]\n",
    "\n",
    "min_samples_leaf_list = [1, 2, 5]\n",
    "criterion_list  = [\"gini\", \"entropy\"]\n",
    "\n",
    "\n",
    "# accuracy_vector_extra_tree = -100 + np.zeros((len(n_estimators_list), len(min_samples_split_list), len(max_depth_list), \n",
    "#                                               len(min_samples_leaf_list), len(criterion_list) ))\n",
    "\n",
    "# for i1 in range(len(n_estimators_list)):\n",
    "#     for i2 in range(len(min_samples_split_list)):\n",
    "#         for i3 in range(len(max_depth_list)):\n",
    "#             for i4 in range(len(min_samples_leaf_list)):\n",
    "#                 for i5 in range(len(criterion_list)):\n",
    "\n",
    "#                                 clf = ExtraTreesClassifier( n_estimators = n_estimators_list[i1], \n",
    "#                                                             min_samples_split = min_samples_split_list[i2], \n",
    "#                                                             max_depth = max_depth_list[i3], \n",
    "#                                                             min_samples_leaf = min_samples_leaf_list[i4], \n",
    "#                                                             criterion = criterion_list[i5],  \n",
    "#                                                             random_state = 42)\n",
    "                                \n",
    "#                                 model = clf.fit(X_train, y_train)\n",
    "\n",
    "#                                 # Predict the labels for the test set\n",
    "#                                 y_pred = model.predict(X_test)\n",
    "\n",
    "#                                 # Calculate accuracy\n",
    "#                                 accuracy_vector_extra_tree[i1, i2, i3, i4, i5] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#                                 print(\"#####################\")\n",
    "#                                 print(\"n_estimators\", n_estimators_list[i1])\n",
    "#                                 print(\"min_samples_split\", min_samples_split_list[i2])\n",
    "#                                 print(\"max_depth\", max_depth_list[i3])\n",
    "#                                 print(\"min_samples_leaf\", min_samples_leaf_list[i4])\n",
    "#                                 print(\"criterion\", criterion_list[i5])\n",
    "#                                 print(accuracy_vector_extra_tree[i1, i2, i3, i4, i5])\n",
    "\n",
    "# np.save(\"accuracy_ExtraTree.npy\", accuracy_vector_extra_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vector_ExtraTree = np.load(\"accuracy_ExtraTree.npy\")\n",
    "max_index = np.unravel_index(np.argmax(accuracy_vector_ExtraTree), accuracy_vector_ExtraTree.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946961894953656"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ExtraTreesClassifier( n_estimators = n_estimators_list[max_index[0]], \n",
    "                            min_samples_split = min_samples_split_list[max_index[1]], \n",
    "                            max_depth = max_depth_list[max_index[2]], \n",
    "                            min_samples_leaf = min_samples_leaf_list[max_index[3]], \n",
    "                            criterion = criterion_list[max_index[4]],  )\n",
    "\n",
    "model = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_leaves_list  = [10, 30 , 50]\n",
    "\n",
    "n_estimators_list  = [25, 50, 100, 150]\n",
    "\n",
    "learning_rate_list  = [0.01, 0.1, 1]\n",
    "max_depth_list = [5, 10, 20, 30]\n",
    "\n",
    "lambda_list = [0.0, 1.0, 5.0]\n",
    "boosting_type_list  = [\"gbdt\"]\n",
    "\n",
    "# accuracy_vector = -100 + np.zeros((len(num_leaves_list), len(n_estimators_list), len(learning_rate_list), len(max_depth_list), \n",
    "#                                    len(lambda_list), len(boosting_type_list) ))\n",
    "\n",
    "# for i1 in range(len(num_leaves_list)):\n",
    "#     for i2 in range(len(n_estimators_list)):\n",
    "#         for i3 in range(len(learning_rate_list)):\n",
    "#             for i4 in range(len(max_depth_list)):\n",
    "#                 for i5 in range(len(lambda_list)):\n",
    "#                     for i6 in range(len(boosting_type_list)):\n",
    "\n",
    "#                                 clf = LGBMClassifier(num_leaves = num_leaves_list[i1], \n",
    "#                                                     n_estimators = n_estimators_list[i2], \n",
    "#                                                     learning_rate = learning_rate_list[i3], \n",
    "#                                                     max_depth = max_depth_list[i4], \n",
    "#                                                     reg_lambda = lambda_list[i5], \n",
    "#                                                     boosting_type = boosting_type_list[i6],   \n",
    "#                                                     random_state=42)\n",
    "                                \n",
    "#                                 model = clf.fit(X_train, y_train)\n",
    "\n",
    "#                                 # Predict the labels for the test set\n",
    "#                                 y_pred = model.predict(X_test)\n",
    "\n",
    "#                                 # Calculate accuracy\n",
    "#                                 accuracy_vector[i1, i2, i3, i4, i5, i6] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#                                 print(\"#####################\")\n",
    "#                                 print(\"booster \", boosting_type_list[i6])\n",
    "#                                 print(\"n_estimators \", n_estimators_list[i2])\n",
    "#                                 print(\"learning rate \", learning_rate_list[i3])\n",
    "#                                 print(\"max_depth \", max_depth_list[i4])\n",
    "#                                 print(\"lambda \", lambda_list[i5])\n",
    "#                                 print(\"num_leaves \", num_leaves_list[i1])\n",
    "#                                 print(accuracy_vector[i1, i2, i3, i4, i5, i6])\n",
    "\n",
    "# np.save(\"accuracy_LGBM.npy\", accuracy_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vector_LGBM = np.load(\"accuracy_LGBM.npy\")\n",
    "max_index = np.unravel_index(np.argmax(accuracy_vector_LGBM), accuracy_vector_LGBM.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9495365602471678"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LGBMClassifier(num_leaves = num_leaves_list[max_index[0]], \n",
    "                    n_estimators = n_estimators_list[max_index[1]], \n",
    "                    learning_rate = learning_rate_list[max_index[2]], \n",
    "                    max_depth = max_depth_list[max_index[3]], \n",
    "                    reg_lambda = lambda_list[max_index[4]], \n",
    "                    boosting_type = boosting_type_list[max_index[5]], \n",
    "                    force_col_wise=True, verbose = -1   )\n",
    "\n",
    "model = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Dense(4096, activation='relu'),\n",
    "#     # tf.keras.layers.Dropout(0.3),  # Add dropout with a dropout rate of 0.5 (you can adjust this value)\n",
    "#     tf.keras.layers.Dense(4096, activation='relu'),\n",
    "#     # tf.keras.layers.Dropout(0.3),  # Add dropout with a dropout rate of 0.5 (you can adjust this value)\n",
    "#     tf.keras.layers.Dense(5, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=500) #, validation_data=(X_test, y_test))\n",
    "\n",
    "# model.save('NeuralNet.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('NeuralNet.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 2s 7ms/step - loss: 0.6976 - accuracy: 0.9413\n",
      "Test accuracy: 0.9413\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
